---
title: "hw3_yt2785"
output: github_document
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggridges)
library(tidyverse)
library(patchwork)
library(viridis)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6
,
out.width = "90%"
)
```

**Problem 1**

##load the `instacart` data and tidy the name.

```{r message=FALSE, warning=FALSE}
library(p8105.datasets) 
data("instacart")
instacart_df = 
  instacart %>% 
  janitor::clean_names()

n_row_instacart = nrow(instacart_df)
n_col_instacart = ncol(instacart_df)
name_instacart = colnames(instacart_df)

instacart_df %>% 
  distinct(department) %>%
    summarize(n_department = n())
instacart_df %>%
  distinct(aisle) %>% 
  summarize(n_aisle = n())

```
**Destribution: This data set includes `r n_row_instacart` observations and `r n_col_instacart` variables. The name of the variables are `r name_instacart`. There are 21 kinds of department and 134 kinds of aisle in total. Only these 2 variables are characters in this data set.**

##How many aisles are there, and which aisles are the most items ordered from?

```{r}
aisle_df = 
  instacart_df %>%
  group_by(aisle) %>% 
  summarize(n_obs = n())
n_row_aisle = nrow(aisle_df)


aisle_most_df = 
  aisle_df %>% 
  mutate(aisle_rank = min_rank(desc(n_obs))) %>% 
  filter(aisle_rank == 1) 
knitr::kable(aisle_most_df)
```

**There are `r n_row_aisle` aisles in the data frame.The `fresh vegetables` aisles are the most items ordered from.**

##Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

```{r}
instacart_df %>%
  group_by(aisle, department) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
 ggplot(aes(x = reorder(aisle,n_obs), y = n_obs, fill = department)) +
  geom_col() +
  labs(
    title = "number of orders in each aisle",
    x = "aisles",
    y = "number of orders"
  ) + 
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 1), 
        axis.text = element_text(size = 8),
        legend.position = "bottom",
        legend.text = element_text(size = 8)) + 
  coord_flip()
```

**The x-axis represents the name of the aisles, and the y-axis represents the number of the aisles. From this plot we can found that the number of `Fresh Vegetables` is the largest and the number of `butter` is the smallest.** 

##Make a table showing the three most popular items in each of the aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`.

```{r}
most_popular_item = 
  instacart_df %>% 
  filter(aisle %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_times = n()) %>% 
  mutate(product_rank = min_rank(desc(n_times))) %>% 
  filter(product_rank == 1) %>% 
  select(-product_rank)
  knitr::kable(most_popular_item)
```


##Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r}
mean_hour_df = 
  instacart_df %>%
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>%
  mutate(order_dow = recode(order_dow, '0' = "Sunday",'1' = "Monday", '2' = "Tuesday", '3' = "Wednesday", '4' = "Thursday", '5' = "Friday", '6' = "Saturday")) %>% 
   pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) 

knitr::kable(mean_hour_df)
```

**Problem 2**

##load the brfss data and clean it

```{r}
data("brfss_smart2010")

brfss = 
  brfss_smart2010 %>%
  janitor::clean_names() %>% 
    rename(state = locationabbr) %>% 
  filter(topic %in% c("Overall Health")) %>% 
  filter(response %in% c("Excellent","Very good","Fair","Good","Poor")) %>% 
  mutate(
    response_rank = recode(response, `Poor` = "1", `Fair` = "2", `Good` = "3", `Very good` = "4", `Excellent` = "5"),
    response = fct_reorder(response, response_rank))

brfss
```


##In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r}
brfss_2002 = 
  brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  distinct(locationdesc) %>% 
  summarize(n_state_2002 = n()) %>% 
  filter(n_state_2002 > 6)

brfss_2010 = 
  brfss %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  distinct(locationdesc) %>% 
  summarize(n_state_2010 = n()) %>% 
  filter(n_state_2010 > 6) 

brfss_2002
brfss_2010

name_2002 = pull(brfss_2002, state)
name_2010 = pull(brfss_2010, state)
```

**The states that were observed at 7 or more locations in 2002 are `r name_2002`. The states that were observed at 7 or more locations in 2010 are `r name_2010`.**

##Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 

```{r}
excellent_df = 
  brfss %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(mean_data_value = mean(data_value, na.rm = TRUE)) 
excellent_df
```

##Make a `spaghetti` plot of this average value over time within a state 

```{r}
ggplot(data = excellent_df, aes(x = year, y = mean_data_value, color = state)) +
  geom_line(aes(group = state))
```

##Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (`Poor` to `Excellent`) among locations in NY State.

```{r}
brfss %>% 
  group_by(year, state) %>% 
  filter((year == 2006 | year == 2010),
         state == "NY") %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  labs(title = "Distribution of data_value in NY (2006 and 2010)") +
  facet_grid(. ~ year)
```

**Problem 3**


##Load, tidy, and otherwise wrangle the data. 

```{r}
accel_df = 
  read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekday_vs_weekend = recode(day, 'Monday' = "weekday",'Tuesday' = "weekday", 'Wednesday' = "weekday", 'Thursday' = "weekday", 'Friday' = "weekday", 'Saturday' = "weekend", 'Sunday' = "weekend")) %>% 
  select(week, day_id, day, weekday_vs_weekend, everything()) %>% 
  pivot_longer(
    activity_1 : activity_1440,
    names_to = "time_minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(time_minute = as.numeric(time_minute))

name_accel = colnames(accel_df)
n_col_accel = ncol(accel_df)
n_row_accel = nrow(accel_df)
```
**There are `r n_col_accel` variebles exist, which are `r name_accel`. And there are `r n_row_accel` observations.  All of the minute and their activity count is linked together clearly.**


##aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals.

```{r}
select_activity_df = 
accel_df %>% 
  group_by(day_id, week, day) %>% 
  summarize(total = sum(activity_count)) 

knitr::kable(select_activity_df)
```
**It seems that in the same week, the total activity count increase from Monday to Sunday.But in the 4th and 5th week, the total activity count is abnormally low on Saturday.**

## Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r}
accel_df %>% 
  ggplot(aes(x = time_minute, y = activity_count, color = day)) + 
  geom_line() +
  labs(
    title = "24h activity",
    x = "Time",
    y = "Activity Count"
  ) +
  scale_x_continuous(breaks = c(0, 120, 240 ,360, 480, 600, 720, 840, 960,1080, 1200, 1320, 1440), 
                   labels = c("12am","2am", "4am", "6am", "8am", "10am", "12pm", "2pm", "4pm", "6pm", "8pm", "10pm", "12am"),
                   limits = c(0, 1440))

```

**The highest activity count always appear at 8pm everyday, and it's always the lowest at 12am everyday. The height of activity always increase gradually from 12am to 12pm, and always decrease gradually from 10pm to 12am. 12pm is also a small peak of activity count. These mean that the man will take relax from 10pm to 8am in the next day, and will have the most active time at 8pm everyday.**
